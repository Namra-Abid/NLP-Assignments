{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a921a793-2a62-4b2e-8339-8b4dca8b7aab",
   "metadata": {},
   "source": [
    "## Question1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d475ce-cd84-46e0-89f7-d5de68f4364e",
   "metadata": {},
   "source": [
    "##### a) Implement FastText and BERT Embeddings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "338e14bc-0828-488c-a736-f0b9880b1bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1a0f038-ceeb-4523-a456-36b8fc874ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NamraAbid\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded to: C:\\Users\\NamraAbid\\.cache\\huggingface\\hub\\models--facebook--fasttext-en-vectors\\snapshots\\a80392390daaee1a91000da45c376d512e1dc555\\model.bin\n",
      "Model file size: 6901.91 MB\n",
      "Model loaded successfully!\n",
      "[ 1.57576188e-01  4.37820926e-02 -4.51271934e-03  6.65931404e-02\n",
      "  7.70346820e-02  4.85855248e-03  8.19822028e-03  6.52402919e-03\n",
      "  9.25899856e-03  3.53899002e-02 -2.31395271e-02 -4.91807126e-02\n",
      " -8.32642540e-02  1.56014524e-02  2.54856616e-01  3.45423706e-02\n",
      " -1.07451361e-02 -7.80188590e-02 -7.08099529e-02  7.62385577e-02\n",
      " -6.09613657e-02  4.48625796e-02 -7.29744136e-02  1.30583309e-02\n",
      "  3.14881057e-02 -3.10055036e-02  1.66004002e-02  1.74405202e-02\n",
      " -7.35838860e-02  1.18252613e-01 -1.21330231e-01 -4.09253240e-02\n",
      "  2.93969568e-02  4.84445989e-02 -1.33816330e-02 -1.74765270e-02\n",
      "  7.51308873e-02  9.97046307e-02 -4.00476977e-02  4.05735290e-03\n",
      " -7.21896589e-02 -4.43356819e-02 -1.22628408e-03  7.56693557e-02\n",
      "  3.98401320e-02  3.22643593e-02  1.95914153e-02  4.68016043e-02\n",
      " -1.46228177e-02  1.12967767e-01  3.15065160e-02 -1.02312110e-01\n",
      "  1.58124104e-01 -2.76147053e-02 -3.39851156e-02 -1.77006852e-02\n",
      " -5.73529862e-04  1.10789239e-01 -1.64533369e-02 -3.14955460e-03\n",
      " -4.22914140e-02  1.11429848e-01 -5.31049855e-02  4.91117276e-02\n",
      "  9.10004079e-02  6.57141507e-02 -3.71061601e-02  3.81702930e-02\n",
      "  7.25173131e-02 -5.31874336e-02  3.06243524e-02 -5.77391349e-02\n",
      " -8.07492957e-02 -9.05582383e-02 -8.05390999e-02 -6.03040010e-02\n",
      " -9.73476470e-02  4.83466834e-02  6.79628998e-02 -2.63391621e-03\n",
      " -8.63242708e-03 -5.09856315e-03  3.15496624e-02  6.66525513e-02\n",
      "  3.12875141e-04 -8.35073516e-02  4.45498899e-02  3.60494666e-02\n",
      " -2.06746310e-02 -6.20845817e-02 -9.07698199e-02 -4.88502011e-02\n",
      "  1.32845968e-01  1.26201622e-02  4.61448133e-02 -5.53582981e-02\n",
      "  2.26286706e-03  4.92154472e-02  3.35916355e-02  6.64286166e-02\n",
      " -8.92760456e-02 -5.37227653e-02  1.32202283e-01 -9.05150920e-03\n",
      "  3.26110516e-03 -4.37462777e-02  7.51723945e-02 -4.36847992e-02\n",
      " -3.93423960e-02  4.89794314e-02  8.05674866e-02 -3.93629894e-02\n",
      " -7.60222226e-02  7.16625601e-02 -1.88665651e-02 -4.20744009e-02\n",
      "  3.32255103e-03 -2.13907361e-02 -1.30127341e-01  1.37401130e-02\n",
      " -5.14834598e-02  3.86724435e-02  4.92810011e-02 -6.17840439e-02\n",
      " -3.39861885e-02  3.51758078e-02  2.59123407e-02 -1.02832042e-01\n",
      "  6.01336509e-02 -7.14224055e-02 -2.23655030e-02 -1.03390224e-01\n",
      " -6.34965971e-02  1.22897769e-03 -8.42045806e-03 -7.10138381e-02\n",
      " -1.38788186e-02  9.29828510e-02 -7.62190223e-02 -1.79991737e-01\n",
      "  4.98081669e-02  5.59808277e-02  4.36702892e-02  1.68789774e-02\n",
      " -3.51566449e-02  5.45868883e-03 -1.51729390e-01  8.31367448e-03\n",
      "  1.33901536e-01  1.18388735e-01 -2.54749060e-02 -5.89675866e-02\n",
      " -1.15508147e-01 -9.11533982e-02 -3.26217338e-02  9.58938058e-03\n",
      "  7.08419904e-02 -1.19613513e-01 -2.44825650e-02  4.67297807e-02\n",
      " -1.05831511e-01  8.39347020e-03 -3.59367356e-02 -7.11603984e-02\n",
      "  1.49144500e-01 -9.40826610e-02  3.87760401e-02  4.80452590e-02\n",
      "  2.00118758e-02  5.70331514e-02 -5.09383976e-02 -1.54985264e-02\n",
      " -3.21162455e-02  6.39992654e-02  4.45546657e-02 -5.41638955e-02\n",
      "  2.38869134e-02  3.99200059e-02  4.95060384e-02 -8.13021213e-02\n",
      "  8.67957771e-02  2.78793890e-02  2.23497916e-02  6.88121617e-02\n",
      "  5.80286458e-02  1.24275330e-02  9.18484554e-02  1.70225650e-02\n",
      " -2.20671259e-02 -5.54737449e-02  3.15260515e-03 -8.95306170e-02\n",
      " -5.89935109e-04 -4.80783619e-02 -4.11259457e-02 -3.47180255e-02\n",
      " -4.23192009e-02  1.01052016e-01  4.34643961e-02  6.75219819e-02\n",
      " -7.32917935e-02  2.32507251e-02  3.76763381e-02  9.02093761e-03\n",
      " -8.25045630e-02 -9.67509300e-02  5.91404364e-03  2.62256898e-02\n",
      " -2.22521871e-02  7.38612264e-02 -1.88500714e-03 -9.77522582e-02\n",
      " -5.37980236e-02 -4.76639681e-02 -1.30426334e-02  8.38671811e-04\n",
      "  2.90181581e-02 -3.12499143e-03 -9.28533375e-02  6.73858598e-02\n",
      " -1.85458809e-01  4.01153788e-02 -5.62882163e-02  6.18898645e-02\n",
      "  8.93600285e-02 -6.91142231e-02 -3.22221480e-02 -1.35385573e-01\n",
      " -7.45606720e-02  1.01488158e-01 -2.72288243e-03  6.07009046e-02\n",
      "  2.42582299e-02 -1.51890054e-01 -2.93815900e-02 -4.21775132e-03\n",
      "  5.16449586e-02  1.85986951e-01 -2.56413780e-02  8.12229067e-02\n",
      "  3.16283293e-03 -3.35572846e-02  3.90090160e-02 -7.37856179e-02\n",
      "  1.14605539e-01 -7.38329254e-05 -3.69094908e-02  9.31020677e-02\n",
      " -2.92852186e-02  5.21238521e-02  7.99705926e-03 -2.93293986e-02\n",
      "  1.31182939e-01 -8.32130760e-02 -3.40401530e-02  1.21310152e-01\n",
      "  3.51337232e-02  4.17837035e-03  5.03289811e-02  2.06086487e-02\n",
      "  7.90461749e-02 -4.95089963e-02  2.54211240e-02 -2.95754354e-02\n",
      " -2.65460461e-02  5.42523079e-02 -5.52508160e-02  1.06864944e-02\n",
      " -3.00089158e-02 -6.05286062e-02  8.54329094e-02 -6.65596053e-02\n",
      " -6.78129196e-02  3.51911336e-02  6.19770139e-02  4.80552204e-02\n",
      " -3.45021002e-02 -2.87248623e-02 -5.90536669e-02 -5.05724642e-03\n",
      " -9.74042267e-02  1.88945048e-03 -9.06497836e-02  1.47764226e-02\n",
      " -9.77678970e-02  3.95894758e-02  2.82567330e-02 -9.28364843e-02\n",
      " -8.16594157e-03 -4.56805378e-02  1.12314738e-01  8.59746262e-02\n",
      " -1.47517517e-01  8.33301097e-02  9.94666740e-02 -3.67202386e-02\n",
      "  6.84743300e-02  8.06697235e-02 -4.50269580e-02 -3.11294980e-02]\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "try:\n",
    "    # Download the model and get the local path\n",
    "    model_path = hf_hub_download(repo_id=\"facebook/fasttext-en-vectors\", filename=\"model.bin\")\n",
    "    print(f\"Model downloaded to: {model_path}\")\n",
    "\n",
    "    # Verify file exists and check its size\n",
    "    if os.path.exists(model_path):\n",
    "        file_size = os.path.getsize(model_path) / (1024 * 1024)  # Size in MB\n",
    "        print(f\"Model file size: {file_size:.2f} MB\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "\n",
    "    # Load the model\n",
    "    model_ft = fasttext.load_model(model_path)\n",
    "    print(\"Model loaded successfully!\")\n",
    "\n",
    "    \n",
    "    print(model_ft.get_word_vector(\"hello\"))\n",
    "except MemoryError:\n",
    "    print(\"MemoryError: Not enough RAM to load the model. Try:\")\n",
    "    print(\"- Closing other applications to free up memory\")\n",
    "    print(\"- Increasing virtual memory in Windows settings\")\n",
    "    print(\"- Using a smaller model or a machine with more RAM\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4cb9d60-878a-42dc-a6e5-086cd8900d01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText Embedding for first sentence: [ 0.0445471   0.0264558  -0.0020836   0.04669745 -0.01345486] ...\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Cats are great pets.\",\n",
    "    \"Dogs are good companions.\",\n",
    "    \"I love programming in Python.\",\n",
    "    \"Snakes are not very affectionate.\"\n",
    "]\n",
    "fasttext_embeddings = []\n",
    "if model_ft:\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        word_embeddings = [model_ft.get_word_vector(word) for word in words]\n",
    "        sentence_embedding = np.mean(word_embeddings, axis=0) if word_embeddings else np.zeros(model_ft.get_dimension())\n",
    "        fasttext_embeddings.append(sentence_embedding)\n",
    "    print(\"FastText Embedding for first sentence:\", fasttext_embeddings[0][:5], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7935ce4a-0d09-49e4-a0ec-e1b064ffe818",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c9910456-f4c8-4579-a04b-6613f9009025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model and tokenizer loaded successfully!\n",
      "BERT Embedding for first sentence: [ 0.20707242  0.14218685 -0.3380488  -0.14978017 -0.43513113] ...\n",
      "SBERT model loaded successfully!\n",
      "SBERT Embedding for first sentence: [ 0.07553703 -0.00508587  0.0994834   0.00859571 -0.13170868] ...\n",
      "\n",
      "Sentence-level Cosine Similarities:\n",
      "s1 vs s2: FastText=0.8387, BERT=0.9628, SBERT=0.5809\n",
      "s1 vs s3: FastText=0.3417, BERT=0.9100, SBERT=0.2578\n",
      "s1 vs s4: FastText=0.6859, BERT=0.8899, SBERT=0.3806\n",
      "s2 vs s3: FastText=0.3474, BERT=0.8719, SBERT=0.1761\n",
      "s2 vs s4: FastText=0.7787, BERT=0.8939, SBERT=0.2572\n",
      "s3 vs s4: FastText=0.4551, BERT=0.8401, SBERT=0.1329\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 1a. BERT Embeddings\n",
    "try:\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    print(\"BERT model and tokenizer loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"BERT error: {e}\")\n",
    "    tokenizer, bert_model = None, None\n",
    "\n",
    "bert_embeddings = []\n",
    "if bert_model and tokenizer:\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(**inputs)\n",
    "        sentence_embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "        bert_embeddings.append(sentence_embedding[0])\n",
    "    print(\"BERT Embedding for first sentence:\", bert_embeddings[0][:5], \"...\")\n",
    "\n",
    "# 1b. SBERT Embeddings\n",
    "try:\n",
    "    sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    print(\"SBERT model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"SBERT error: {e}\")\n",
    "    sbert_model = None\n",
    "\n",
    "sbert_embeddings = []\n",
    "if sbert_model:\n",
    "    sbert_embeddings = sbert_model.encode(texts)\n",
    "    print(\"SBERT Embedding for first sentence:\", sbert_embeddings[0][:5], \"...\")\n",
    "\n",
    "# 1b. Calculate Cosine Similarity for All Pairs\n",
    "if len(fasttext_embeddings) > 0 and len(bert_embeddings) > 0 and len(sbert_embeddings) > 0:\n",
    "    print(\"\\nSentence-level Cosine Similarities:\")\n",
    "    for i in range(len(texts)):\n",
    "        for j in range(i + 1, len(texts)):\n",
    "            fasttext_sim = util.cos_sim(fasttext_embeddings[i], fasttext_embeddings[j]).item()\n",
    "            bert_sim = util.cos_sim(bert_embeddings[i], bert_embeddings[j]).item()\n",
    "            sbert_sim = util.cos_sim(sbert_embeddings[i], sbert_embeddings[j]).item()\n",
    "            print(f\"s{i+1} vs s{j+1}: FastText={fasttext_sim:.4f}, BERT={bert_sim:.4f}, SBERT={sbert_sim:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c39291-8faa-438b-b9fb-b8247f22257b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7a6c1db6-8e79-45a3-9d9a-66ab1c6fa86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e6d209-2881-4844-910c-e0430cd82a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5b38fd08-290d-4732-a262-a4b595f7a4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy model loaded successfully!\n",
      "2a. Original sentences:\n",
      "s1: In June 2022, Dr\n",
      "s2: Andrew Scott, a world-leading epidemiologist, delivered a keynote address at the World Health Organization (WHO) Headquarters in Geneva, Switzerland\n",
      "s3: His presentation, titled “The Silent Wave: Climate Change and Emerging Health Crises,” emphasized the interconnectedness of environmental shifts and global health\n",
      "s4: Dr\n",
      "s5: Scott outlined how rising temperatures exacerbate the spread of diseases like malaria and dengue fever while also affecting mental health due to increased natural disasters\n",
      "s6: During the conference, he engaged in a collaborative workshop with Prof\n",
      "s7: Lin Mei, an environmental scientist, and Dr\n",
      "s8: Rajesh Kumar, a medical economist\n",
      "s9: Together, they developed a framework to predict and mitigate future health crises using machine learning algorithms to analyze environmental and epidemiological data\n",
      "s10: Later that evening, Andrew participated in a panel discussion at the Graduate Institute of International and Development Studies, addressing the need for policy reform to ensure equitable access to healthcare resources in climate-affected regions\n",
      "s11: After a productive day, he spent time by Lake Geneva, reflecting on the urgency of integrating health and climate policy on a global scale\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy model \n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    print(\"spaCy model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"spaCy error: {e}\")\n",
    "    nlp = None\n",
    "\n",
    "def preprocess_text(doc):\n",
    "    preprocessed = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return ' '.join(preprocessed)\n",
    "\n",
    "# 2a. Load newspaper text \n",
    "news_text = \"\"\"\n",
    "   In June 2022, Dr. Andrew Scott, a world-leading epidemiologist, delivered a keynote address at the World Health Organization (WHO) Headquarters in Geneva, Switzerland. \n",
    "   His presentation, titled “The Silent Wave: Climate Change and Emerging Health Crises,” emphasized the interconnectedness of environmental shifts and global health. \n",
    "   Dr. Scott outlined how rising temperatures exacerbate the spread of diseases like malaria and dengue fever while also affecting mental health due to increased natural disasters.\n",
    "\n",
    "During the conference, he engaged in a collaborative workshop with Prof. Lin Mei, an environmental scientist, and Dr. Rajesh Kumar, a medical economist.\n",
    "Together, they developed a framework to predict and mitigate future health crises using machine learning algorithms to analyze environmental and epidemiological data.\n",
    "Later that evening, Andrew participated in a panel discussion at the Graduate Institute of International and Development Studies, addressing the need for policy reform to ensure equitable access to healthcare resources in climate-affected regions.\n",
    "\n",
    "After a productive day, he spent time by Lake Geneva, reflecting on the urgency of integrating health and climate policy on a global scale.\n",
    "\"\"\"\n",
    "sentences = news_text.split('.')\n",
    "sentences = [s.strip() for s in sentences if s.strip()]  # Split into sentences, remove empty\n",
    "print(\"2a. Original sentences:\")\n",
    "for i, s in enumerate(sentences):\n",
    "    print(f\"s{i+1}: {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f2376aec-1e55-49d2-806a-65830412e82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT model loaded successfully!\n",
      "\n",
      "2b. Clustering with original text:\n",
      "Sentence: june 2022 dr -> Cluster 1\n",
      "Sentence: andrew scott world lead epidemiologist deliver keynote address world health organization headquarter geneva switzerland -> Cluster 0\n",
      "Sentence: presentation title silent wave climate change emerging health crises emphasize interconnectedness environmental shift global health -> Cluster 0\n",
      "Sentence: dr -> Cluster 1\n",
      "Sentence: scott outline rise temperature exacerbate spread disease like malaria dengue fever affect mental health increase natural disaster -> Cluster 0\n",
      "Sentence: conference engage collaborative workshop prof -> Cluster 0\n",
      "Sentence: lin mei environmental scientist dr -> Cluster 1\n",
      "Sentence: rajesh kumar medical economist -> Cluster 1\n",
      "Sentence: develop framework predict mitigate future health crisis machine learn algorithm analyze environmental epidemiological datum -> Cluster 0\n",
      "Sentence: later evening andrew participate panel discussion graduate institute international development studies address need policy reform ensure equitable access healthcare resource climate affect region -> Cluster 0\n",
      "Sentence: productive day spend time lake geneva reflect urgency integrate health climate policy global scale -> Cluster 0\n",
      "Silhouette Score (higher is better, range -1 to 1): 0.10471121966838837\n"
     ]
    }
   ],
   "source": [
    "# 2b. Apply SBERT \n",
    "try:\n",
    "    sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    print(\"SBERT model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"SBERT error: {e}\")\n",
    "    sbert_model = None\n",
    "\n",
    "if sbert_model and nlp:\n",
    "    preprocessed_sentences = [preprocess_text(nlp(s)) for s in sentences]\n",
    "    sbert_embeddings_normal = sbert_model.encode(preprocessed_sentences)\n",
    "    # Cluster with KMeans (2 clusters for simplicity)\n",
    "    kmeans_normal = KMeans(n_clusters=2, random_state=42)\n",
    "    normal_clusters = kmeans_normal.fit_predict(sbert_embeddings_normal)\n",
    "    # Compute silhouette score to assess cluster quality\n",
    "    if len(set(normal_clusters)) > 1:  # Silhouette requires 2+ clusters\n",
    "        sil_score_normal = silhouette_score(sbert_embeddings_normal, normal_clusters)\n",
    "    else:\n",
    "        sil_score_normal = \"N/A (too few clusters)\"\n",
    "    print(\"\\n2b. Clustering with original text:\")\n",
    "    for i, s in enumerate(preprocessed_sentences):\n",
    "        print(f\"Sentence: {s} -> Cluster {normal_clusters[i]}\")\n",
    "    print(f\"Silhouette Score (higher is better, range -1 to 1): {sil_score_normal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4d19c69f-0440-4c6d-aa10-200f0ba5bda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2c. Clustering after removing named entities:\n",
      "Sentence: dr : C:luster 1\n",
      "Sentence: world lead epidemiologist deliver keynote address world health organization headquarter : C:luster 1\n",
      "Sentence: presentation title silent wave climate change emerging health crises emphasize interconnectedness environmental shift global health : C:luster 0\n",
      "Sentence: dr : C:luster 1\n",
      "Sentence: scott outline rise temperature exacerbate spread disease like malaria dengue fever affect mental health increase natural disaster : C:luster 0\n",
      "Sentence: conference engage collaborative workshop prof : C:luster 0\n",
      "Sentence: environmental scientist dr : C:luster 1\n",
      "Sentence: medical economist : C:luster 1\n",
      "Sentence: develop framework predict mitigate future health crisis use machine learn algorithm analyze environmental epidemiological datum : C:luster 0\n",
      "Sentence: later evening participate panel discussion graduate institute international development studies address need policy reform ensure equitable access healthcare resource climate affect region : C:luster 0\n",
      "Sentence: productive day spend time lake geneva reflect urgency integrate health climate policy global scale : C:luster 0\n",
      "Silhouette Score (higher is better, range -1 to 1): 0.14749452471733093\n",
      "\n",
      "2c. Visualizing named entities in original text:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">In \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    June 2022\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Dr\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Andrew Scott\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", a world-leading epidemiologist, delivered a keynote address at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the World Health Organization (WHO\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ") Headquarters in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Geneva\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Switzerland\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">His presentation, titled “\n",
       "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The Silent Wave: Climate Change and Emerging Health Crises\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
       "</mark>\n",
       ",” emphasized the interconnectedness of environmental shifts and global health</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NamraAbid\\AppData\\Roaming\\Python\\Python311\\site-packages\\spacy\\displacy\\__init__.py:213: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
      "  warnings.warn(Warnings.W006)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Dr</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Scott\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " outlined how rising temperatures exacerbate the spread of diseases like malaria and dengue fever while also affecting mental health due to increased natural disasters</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">During the conference, he engaged in a collaborative workshop with \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Prof\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Lin Mei\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", an environmental scientist, and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Dr\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Rajesh Kumar\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", a medical economist</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Together, they developed a framework to predict and mitigate future health crises using machine learning algorithms to analyze environmental and epidemiological data</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Later that evening, \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Andrew\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " participated in a panel discussion at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Graduate Institute of International and Development Studies\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", addressing the need for policy reform to ensure equitable access to healthcare resources in climate-affected regions</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">After a productive day, he spent time by \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Lake Geneva\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", reflecting on the urgency of integrating health and climate policy on a global scale</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2c. Remove named entities (time, places, people) using spaCy\n",
    "if nlp and sbert_model:\n",
    "    sentences_no_entities = []\n",
    "    for s in sentences:\n",
    "        doc = nlp(s)\n",
    "        # Remove entities: PERSON, GPE (places), DATE, TIME\n",
    "        filtered_tokens = [token.lemma_ for token in doc if token.ent_type_ not in ('PERSON', 'GPE', 'DATE', 'TIME')]\n",
    "        filtered_text = ' '.join(filtered_tokens)\n",
    "        sentences_no_entities.append(preprocess_text(nlp(filtered_text)))\n",
    "    sbert_embeddings_no_entities = sbert_model.encode(sentences_no_entities)\n",
    "    # Cluster\n",
    "    kmeans_no_entities = KMeans(n_clusters=2, random_state=42)\n",
    "    no_entities_clusters = kmeans_no_entities.fit_predict(sbert_embeddings_no_entities)\n",
    "    # Silhouette score\n",
    "    if len(set(no_entities_clusters)) > 1:\n",
    "        sil_score_no_entities = silhouette_score(sbert_embeddings_no_entities, no_entities_clusters)\n",
    "    else:\n",
    "        sil_score_no_entities = \"N/A (too few clusters)\"\n",
    "    print(\"\\n2c. Clustering after removing named entities:\")\n",
    "    for i, s in enumerate(sentences_no_entities):\n",
    "        print(f\"Sentence: {s} : C:luster {no_entities_clusters[i]}\")\n",
    "    print(f\"Silhouette Score (higher is better, range -1 to 1): {sil_score_no_entities}\")\n",
    "    # Visualize named entities in original text with displacy\n",
    "    print(\"\\n2c. Visualizing named entities in original text:\")\n",
    "    for s in sentences:\n",
    "        doc = nlp(s)\n",
    "        from spacy import displacy\n",
    "        displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1303aec1-e705-45f6-b3ce-9ead4912b70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2d. Clustering after removing all nouns:\n",
      "Sentence: 2022 -> Cluster 1\n",
      "Sentence: lead deliver keynote -> Cluster 1\n",
      "Sentence: title emphasize environmental global -> Cluster 1\n",
      "Sentence:  -> Cluster 1\n",
      "Sentence: outline rise exacerbate like dengue affect mental increase natural -> Cluster 0\n",
      "Sentence: engage collaborative -> Cluster 1\n",
      "Sentence: environmental -> Cluster 1\n",
      "Sentence: medical -> Cluster 0\n",
      "Sentence: develop predict mitigate future use learn analyze environmental epidemiological -> Cluster 0\n",
      "Sentence: later participate address ensure equitable affect -> Cluster 1\n",
      "Sentence: productive spend reflect integrate global -> Cluster 1\n",
      "Silhouette Score (higher is better, range -1 to 1): 0.03165236860513687\n"
     ]
    }
   ],
   "source": [
    "# 2d. Remove all nouns using spaCy\n",
    "if nlp and sbert_model:\n",
    "    sentences_no_nouns = []\n",
    "    for s in sentences:\n",
    "        doc = nlp(s)\n",
    "        # Remove nouns (NOUN, PROPN)\n",
    "        filtered_tokens = [token.lemma_ for token in doc if token.pos_ not in ('NOUN', 'PROPN')]\n",
    "        filtered_text = ' '.join(filtered_tokens)\n",
    "        sentences_no_nouns.append(preprocess_text(nlp(filtered_text)))\n",
    "    sbert_embeddings_no_nouns = sbert_model.encode(sentences_no_nouns)\n",
    "    # Cluster\n",
    "    kmeans_no_nouns = KMeans(n_clusters=2, random_state=42)\n",
    "    no_nouns_clusters = kmeans_no_nouns.fit_predict(sbert_embeddings_no_nouns)\n",
    "    # Silhouette score\n",
    "    if len(set(no_nouns_clusters)) > 1:\n",
    "        sil_score_no_nouns = silhouette_score(sbert_embeddings_no_nouns, no_nouns_clusters)\n",
    "    else:\n",
    "        sil_score_no_nouns = \"N/A (too few clusters)\"\n",
    "    print(\"\\n2d. Clustering after removing all nouns:\")\n",
    "    for i, s in enumerate(sentences_no_nouns):\n",
    "        print(f\"Sentence: {s} -> Cluster {no_nouns_clusters[i]}\")\n",
    "    print(f\"Silhouette Score (higher is better, range -1 to 1): {sil_score_no_nouns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0452071-e2d3-43dd-ade8-7a7307153efd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bf3965-58ba-4eab-b488-b47a1e865ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "10de93f5-d532-4146-bd34-a6eb9b584794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "629a9b7b-5b43-4504-80c6-a9f9c347e44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster assignments:\n",
      "    Sentence  Original  No Entities  No Nouns\n",
      "0          0         1            1         1\n",
      "1          1         0            1         1\n",
      "2          2         0            0         1\n",
      "3          3         1            1         1\n",
      "4          4         0            0         0\n",
      "5          5         0            0         1\n",
      "6          6         1            1         1\n",
      "7          7         1            1         0\n",
      "8          8         0            0         0\n",
      "9          9         0            0         1\n",
      "10        10         0            0         1\n"
     ]
    }
   ],
   "source": [
    "# 2e. Compare results\n",
    "\n",
    "if normal_clusters is not None and no_entities_clusters is not None and no_nouns_clusters is not None:\n",
    "    # Create DataFrame for cluster distributions\n",
    "    cluster_data = pd.DataFrame({\n",
    "        'Sentence': range(len(sentences)),\n",
    "        'Original': normal_clusters,\n",
    "        'No Entities': no_entities_clusters,\n",
    "        'No Nouns': no_nouns_clusters\n",
    "    })\n",
    "    print(\"\\nCluster assignments:\")\n",
    "    print(cluster_data)\n",
    "\n",
    " \n",
    "    cluster_melted = pd.melt(cluster_data, id_vars=['Sentence'], \n",
    "                             value_vars=['Original', 'No Entities', 'No Nouns'], \n",
    "                             var_name='Condition', value_name='Cluster')\n",
    "    \n",
    "    \n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
